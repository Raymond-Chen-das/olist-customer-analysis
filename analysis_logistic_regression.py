# ====================================================
# é‚è¼¯æ–¯å›æ­¸åˆ†æ (Logistic Regression)
# é æ¸¬é¡§å®¢æ˜¯å¦æœƒå›è³¼
# ====================================================
import os
import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy.stats import chi2_contingency
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (classification_report, confusion_matrix,
                             roc_curve, auc, precision_recall_curve)
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['Microsoft JhengHei']
matplotlib.rcParams['axes.unicode_minus'] = False

# è¼‰å…¥å…±ç”¨è³‡æ–™æ¨¡çµ„
from data_preparation import load_and_prepare_data, get_output_folder

print("="*60)
print("ğŸ“Š é‚è¼¯æ–¯å›æ­¸åˆ†æ (Logistic Regression)")
print("="*60)

# è¼‰å…¥è³‡æ–™
customer_features = load_and_prepare_data()
output_folder = get_output_folder()

# ====================================================
# 1. è³‡æ–™æº–å‚™
# ====================================================

print("\n" + "="*60)
print("ğŸ“ˆ 1. è³‡æ–™æº–å‚™")
print("="*60)

# é¸æ“‡é æ¸¬è®Šé …
X_cols = ['avg_delay_days', 'avg_freight_ratio', 'avg_review_score',
          'avg_payment_installments', 'avg_order_value']

X = customer_features[X_cols].copy()
y = customer_features['is_repeat'].copy()

# ç§»é™¤éºæ¼å€¼
valid_idx = ~(X.isna().any(axis=1) | y.isna())
X = X[valid_idx]
y = y[valid_idx]

print(f"æœ‰æ•ˆæ¨£æœ¬æ•¸: {len(y):,}")
print(f"ä¾è®Šé …: æ˜¯å¦å›è³¼ (is_repeat)")
print(f"  - å–®æ¬¡å®¢ (0): {(y==0).sum():,} ({(y==0).mean()*100:.1f}%)")
print(f"  - å›è³¼å®¢ (1): {(y==1).sum():,} ({(y==1).mean()*100:.1f}%)")
print(f"è‡ªè®Šé …: {X_cols}")

# ====================================================
# 2. ä½¿ç”¨ Statsmodels å»ºç«‹é‚è¼¯æ–¯å›æ­¸
# ====================================================

print("\n" + "="*60)
print("ğŸ“ˆ 2. é‚è¼¯æ–¯å›æ­¸æ¨¡å‹ (Statsmodels)")
print("="*60)

# åŠ å…¥å¸¸æ•¸é …
X_const = sm.add_constant(X)

# å»ºç«‹æ¨¡å‹
model_logit = sm.Logit(y, X_const).fit()

print("\n" + "="*50)
print("é‚è¼¯æ–¯å›æ­¸çµæœæ‘˜è¦")
print("="*50)
print(model_logit.summary())

# ====================================================
# 3. å›æ­¸ä¿‚æ•¸èˆ‡å‹ç®—æ¯” (Odds Ratio)
# ====================================================

print("\n" + "="*60)
print("ğŸ“Š 3. å›æ­¸ä¿‚æ•¸èˆ‡å‹ç®—æ¯”è§£è®€")
print("="*60)

# è¨ˆç®—å‹ç®—æ¯”èˆ‡ä¿¡è³´å€é–“
odds_ratios = np.exp(model_logit.params)
conf_int = model_logit.conf_int()
or_ci = np.exp(conf_int)

logit_results = pd.DataFrame({
    'è®Šé …': ['æˆªè· (Intercept)'] + X_cols,
    'ä¿‚æ•¸ (B)': model_logit.params.values,
    'æ¨™æº–èª¤ (SE)': model_logit.bse.values,
    'Wald Ï‡Â²': model_logit.tvalues.values ** 2,
    'p å€¼': model_logit.pvalues.values,
    'å‹ç®—æ¯” (OR)': odds_ratios.values,
    'OR 95% CI ä¸‹ç•Œ': or_ci[0].values,
    'OR 95% CI ä¸Šç•Œ': or_ci[1].values
})

logit_results['é¡¯è‘—æ€§'] = logit_results['p å€¼'].apply(
    lambda p: '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''
)

print("\nğŸ“Š é‚è¼¯æ–¯å›æ­¸ä¿‚æ•¸èˆ‡å‹ç®—æ¯”:")
print(logit_results.to_string(index=False))

print("\nğŸ” é¡¯è‘—é æ¸¬è®Šé …è§£è®€:")
sig_vars = logit_results[(logit_results['é¡¯è‘—æ€§'] != '') & (logit_results['è®Šé …'] != 'æˆªè· (Intercept)')]
for _, row in sig_vars.iterrows():
    or_val = row['å‹ç®—æ¯” (OR)']
    if or_val > 1:
        change = (or_val - 1) * 100
        direction = "å¢åŠ "
    else:
        change = (1 - or_val) * 100
        direction = "æ¸›å°‘"
    print(f"  - {row['è®Šé …']}: OR = {or_val:.3f}")
    print(f"    æ¯å¢åŠ  1 å–®ä½ï¼Œå›è³¼çš„å‹ç®—{direction} {change:.1f}%")

# ====================================================
# 4. æ¨¡å‹é…é©åº¦æª¢é©—
# ====================================================

print("\n" + "="*60)
print("ğŸ“ˆ 4. æ¨¡å‹é…é©åº¦æª¢é©—")
print("="*60)

# Pseudo R-squared
print(f"McFadden's Pseudo RÂ²: {model_logit.prsquared:.4f}")

# Log-Likelihood
print(f"Log-Likelihood: {model_logit.llf:.2f}")
print(f"Log-Likelihood (Null): {model_logit.llnull:.2f}")

# Likelihood Ratio Test
from scipy import stats
lr_stat = -2 * (model_logit.llnull - model_logit.llf)
lr_pvalue = 1 - stats.chi2.cdf(lr_stat, len(X_cols))
print(f"Likelihood Ratio Ï‡Â²: {lr_stat:.2f}, p = {lr_pvalue:.4e}")

# AIC & BIC
print(f"AIC: {model_logit.aic:.2f}")
print(f"BIC: {model_logit.bic:.2f}")

# ====================================================
# 5. ä½¿ç”¨ sklearn é€²è¡Œé æ¸¬è©•ä¼°
# ====================================================

print("\n" + "="*60)
print("ğŸ“ˆ 5. æ¨¡å‹é æ¸¬è©•ä¼° (sklearn)")
print("="*60)

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ†å‰²è³‡æ–™
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)

# å»ºç«‹æ¨¡å‹ (ä½¿ç”¨ class_weight='balanced' è™•ç†ä¸å¹³è¡¡)
lr_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)
lr_model.fit(X_train, y_train)

# é æ¸¬
y_pred = lr_model.predict(X_test)
y_pred_proba = lr_model.predict_proba(X_test)[:, 1]

# åˆ†é¡å ±å‘Š
print("\nğŸ“Š åˆ†é¡å ±å‘Š:")
print(classification_report(y_test, y_pred, target_names=['å–®æ¬¡å®¢', 'å›è³¼å®¢']))

# æ··æ·†çŸ©é™£
cm = confusion_matrix(y_test, y_pred)
print("\nğŸ“Š æ··æ·†çŸ©é™£:")
print(cm)

# äº¤å‰é©—è­‰
cv_scores = cross_val_score(lr_model, X_scaled, y, cv=5, scoring='roc_auc')
print(f"\nğŸ“Š 5-Fold äº¤å‰é©—è­‰ ROC-AUC: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")

# ====================================================
# 6. ROC æ›²ç·šèˆ‡ AUC
# ====================================================

print("\n" + "="*60)
print("ğŸ“Š 6. ROC æ›²ç·šåˆ†æ")
print("="*60)

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

print(f"AUC (Area Under Curve): {roc_auc:.4f}")

# æ‰¾æœ€ä½³é–¾å€¼ (Youden's J)
j_scores = tpr - fpr
best_idx = np.argmax(j_scores)
best_threshold = thresholds[best_idx]
print(f"æœ€ä½³é–¾å€¼ (Youden's J): {best_threshold:.4f}")
print(f"  - æ•æ„Ÿåº¦ (Sensitivity): {tpr[best_idx]:.4f}")
print(f"  - ç‰¹ç•°åº¦ (Specificity): {1-fpr[best_idx]:.4f}")

# ====================================================
# 7. è¦–è¦ºåŒ–
# ====================================================

print("\n" + "="*60)
print("ğŸ“Š 7. ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨")
print("="*60)

# 7.1 å‹ç®—æ¯”æ£®æ—åœ–
or_plot = logit_results[logit_results['è®Šé …'] != 'æˆªè· (Intercept)'].copy()
or_plot['è®Šé …ä¸­æ–‡'] = ['å»¶é²å¤©æ•¸', 'é‹è²»æ¯”ä¾‹', 'è©•è«–åˆ†æ•¸', 'åˆ†æœŸæœŸæ•¸', 'æ¶ˆè²»é‡‘é¡']

fig_or = go.Figure()

# æ·»åŠ ä¿¡è³´å€é–“ç·š
for i, row in or_plot.iterrows():
    fig_or.add_trace(go.Scatter(
        x=[row['OR 95% CI ä¸‹ç•Œ'], row['OR 95% CI ä¸Šç•Œ']],
        y=[row['è®Šé …ä¸­æ–‡'], row['è®Šé …ä¸­æ–‡']],
        mode='lines',
        line=dict(color='#4ECDC4', width=3),
        showlegend=False
    ))

# æ·»åŠ å‹ç®—æ¯”é»
fig_or.add_trace(go.Scatter(
    x=or_plot['å‹ç®—æ¯” (OR)'],
    y=or_plot['è®Šé …ä¸­æ–‡'],
    mode='markers+text',
    marker=dict(size=12, color='#4ECDC4'),
    text=[f"{or_:.3f}{sig}" for or_, sig in zip(or_plot['å‹ç®—æ¯” (OR)'], or_plot['é¡¯è‘—æ€§'])],
    textposition='top center',
    showlegend=False
))

# åƒè€ƒç·š (OR = 1)
fig_or.add_vline(x=1, line_dash="dash", line_color="red", annotation_text="OR=1")

fig_or.update_layout(
    title='<b>é‚è¼¯æ–¯å›æ­¸å‹ç®—æ¯” (Odds Ratio) æ£®æ—åœ–</b><br><sub>OR > 1 è¡¨ç¤ºå¢åŠ å›è³¼æ©Ÿç‡ï¼ŒOR < 1 è¡¨ç¤ºé™ä½å›è³¼æ©Ÿç‡</sub>',
    xaxis_title='å‹ç®—æ¯” (95% CI)',
    yaxis_title='',
    height=450,
    template='plotly_white',
    xaxis=dict(type='log')  # å°æ•¸åˆ»åº¦
)

output_or = os.path.join(output_folder, 'logistic_odds_ratio.html')
fig_or.write_html(output_or)
print(f"âœ… å‹ç®—æ¯”åœ–å·²å„²å­˜: {output_or}")

# 7.2 ROC æ›²ç·š
fig_roc = go.Figure()

fig_roc.add_trace(go.Scatter(
    x=fpr, y=tpr,
    mode='lines',
    name=f'ROC (AUC = {roc_auc:.3f})',
    line=dict(color='#4ECDC4', width=2)
))

fig_roc.add_trace(go.Scatter(
    x=[0, 1], y=[0, 1],
    mode='lines',
    name='éš¨æ©ŸçŒœæ¸¬',
    line=dict(color='gray', dash='dash')
))

# æ¨™è¨˜æœ€ä½³é–¾å€¼é»
fig_roc.add_trace(go.Scatter(
    x=[fpr[best_idx]],
    y=[tpr[best_idx]],
    mode='markers',
    name=f'æœ€ä½³é–¾å€¼ ({best_threshold:.3f})',
    marker=dict(size=12, color='red', symbol='star')
))

fig_roc.update_layout(
    title='<b>ROC æ›²ç·š (Receiver Operating Characteristic)</b>',
    xaxis_title='1 - ç‰¹ç•°åº¦ (False Positive Rate)',
    yaxis_title='æ•æ„Ÿåº¦ (True Positive Rate)',
    height=500,
    template='plotly_white',
    legend=dict(x=0.6, y=0.2)
)

output_roc = os.path.join(output_folder, 'logistic_roc_curve.html')
fig_roc.write_html(output_roc)
print(f"âœ… ROC æ›²ç·šå·²å„²å­˜: {output_roc}")

# 7.3 æ··æ·†çŸ©é™£ç†±åŠ›åœ–
fig_cm = go.Figure(data=go.Heatmap(
    z=cm,
    x=['é æ¸¬: å–®æ¬¡å®¢', 'é æ¸¬: å›è³¼å®¢'],
    y=['å¯¦éš›: å–®æ¬¡å®¢', 'å¯¦éš›: å›è³¼å®¢'],
    colorscale='Blues',
    text=cm,
    texttemplate='%{text}',
    textfont={"size": 20}
))

fig_cm.update_layout(
    title='<b>æ··æ·†çŸ©é™£</b>',
    xaxis_title='é æ¸¬å€¼',
    yaxis_title='å¯¦éš›å€¼',
    height=400,
    template='plotly_white'
)

output_cm = os.path.join(output_folder, 'logistic_confusion_matrix.html')
fig_cm.write_html(output_cm)
print(f"âœ… æ··æ·†çŸ©é™£å·²å„²å­˜: {output_cm}")

# 7.4 Precision-Recall æ›²ç·š
precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)
pr_auc = auc(recall, precision)

fig_pr = go.Figure()

fig_pr.add_trace(go.Scatter(
    x=recall, y=precision,
    mode='lines',
    name=f'PR Curve (AUC = {pr_auc:.3f})',
    line=dict(color='#FF6B6B', width=2)
))

fig_pr.update_layout(
    title='<b>Precision-Recall æ›²ç·š</b>',
    xaxis_title='Recall (æ•æ„Ÿåº¦)',
    yaxis_title='Precision (ç²¾ç¢ºåº¦)',
    height=500,
    template='plotly_white'
)

output_pr = os.path.join(output_folder, 'logistic_pr_curve.html')
fig_pr.write_html(output_pr)
print(f"âœ… PR æ›²ç·šå·²å„²å­˜: {output_pr}")

# ====================================================
# 8. å¡æ–¹æª¢å®š (é¡åˆ¥è®Šé …)
# ====================================================

print("\n" + "="*60)
print("ğŸ“Š 8. å¡æ–¹ç¨ç«‹æ€§æª¢å®š")
print("="*60)

# å›è³¼ç‹€æ…‹ vs ä»˜æ¬¾æ–¹å¼
print("\nğŸ“ˆ 8.1 å›è³¼ç‹€æ…‹ Ã— ä»˜æ¬¾æ–¹å¼:")
contingency_pay = pd.crosstab(customer_features['is_repeat'], customer_features['preferred_payment'])
chi2_pay, p_pay, dof_pay, expected_pay = chi2_contingency(contingency_pay)

print(f"å¡æ–¹å€¼ Ï‡Â² = {chi2_pay:.4f}")
print(f"è‡ªç”±åº¦ df = {dof_pay}")
print(f"p-value = {p_pay:.4e}")
print(f"çµè«–: {'å›è³¼ç‹€æ…‹èˆ‡ä»˜æ¬¾æ–¹å¼æœ‰é¡¯è‘—é—œè¯' if p_pay < 0.05 else 'ç„¡é¡¯è‘—é—œè¯'}")

# CramÃ©r's V (æ•ˆæœé‡)
n = contingency_pay.sum().sum()
min_dim = min(contingency_pay.shape) - 1
cramers_v = np.sqrt(chi2_pay / (n * min_dim))
print(f"CramÃ©r's V = {cramers_v:.4f}")

print("\näº¤å‰è¡¨:")
print(contingency_pay)

# ====================================================
# 9. èˆ‡æ±ºç­–æ¨¹çš„æ¯”è¼ƒ
# ====================================================

print("\n" + "="*60)
print("ğŸ“Š 9. é‚è¼¯æ–¯å›æ­¸ vs æ±ºç­–æ¨¹æ¯”è¼ƒ")
print("="*60)

print("""
ğŸ“‹ æ–¹æ³•æ¯”è¼ƒ:

| ç‰¹æ€§           | é‚è¼¯æ–¯å›æ­¸                | æ±ºç­–æ¨¹                    |
|----------------|---------------------------|---------------------------|
| æ¨¡å‹é¡å‹       | åƒæ•¸æ¨¡å‹                  | éåƒæ•¸æ¨¡å‹                |
| å¯è§£é‡‹æ€§       | ä¿‚æ•¸èˆ‡å‹ç®—æ¯”              | è¦å‰‡èˆ‡åˆ†æ”¯                |
| è™•ç†éç·šæ€§     | éœ€è¦æ‰‹å‹•æ·»åŠ äº¤äº’é …        | è‡ªå‹•æ•æ‰                  |
| éæ“¬åˆé¢¨éšª     | è¼ƒä½                      | è¼ƒé«˜ï¼ˆéœ€å‰ªæï¼‰            |
| ç‰¹å¾µé‡è¦æ€§     | é€éä¿‚æ•¸/OR               | é€é Gini/è³‡è¨Šå¢ç›Š        |
| å‡è¨­           | ç·šæ€§é—œä¿‚                  | ç„¡å‡è¨­                    |
| é©ç”¨æƒ…å¢ƒ       | éœ€è¦è§£é‡‹å› æœé—œä¿‚          | éœ€è¦è¦å‰‡å°å‘æ±ºç­–          |
""")

# ====================================================
# 10. åˆ†ææ‘˜è¦
# ====================================================

print("\n" + "="*60)
print("ğŸ“‹ åˆ†ææ‘˜è¦")
print("="*60)

summary = f"""
{'='*70}
é‚è¼¯æ–¯å›æ­¸åˆ†æå ±å‘Š
{'='*70}

ğŸ“Š ç ”ç©¶å•é¡Œ
-------------------
æ¢è¨å“ªäº›å› ç´ æœƒå½±éŸ¿é¡§å®¢æ˜¯å¦å›è³¼ï¼Ÿ

ğŸ“ˆ æ¨¡å‹è¨­å®š
-------------------
ä¾è®Šé … (Y): æ˜¯å¦å›è³¼ (0=å–®æ¬¡å®¢, 1=å›è³¼å®¢)
è‡ªè®Šé … (X):
  - å¹³å‡å»¶é²å¤©æ•¸ (avg_delay_days)
  - å¹³å‡é‹è²»æ¯”ä¾‹ (avg_freight_ratio)
  - å¹³å‡è©•è«–åˆ†æ•¸ (avg_review_score)
  - å¹³å‡åˆ†æœŸæœŸæ•¸ (avg_payment_installments)
  - å¹³å‡æ¶ˆè²»é‡‘é¡ (avg_order_value)

æ¨£æœ¬æ•¸: {len(y):,}
  - å–®æ¬¡å®¢: {(y==0).sum():,} ({(y==0).mean()*100:.1f}%)
  - å›è³¼å®¢: {(y==1).sum():,} ({(y==1).mean()*100:.1f}%)

ğŸ“Š æ¨¡å‹é…é©åº¦
-------------------
McFadden's Pseudo RÂ²: {model_logit.prsquared:.4f}
AIC: {model_logit.aic:.2f}
BIC: {model_logit.bic:.2f}

ğŸ“ˆ å›æ­¸ä¿‚æ•¸èˆ‡å‹ç®—æ¯”
-------------------
{logit_results.to_string(index=False)}

ğŸ“Š é æ¸¬è©•ä¼°çµæœ
-------------------
ROC-AUC: {roc_auc:.4f}
5-Fold CV ROC-AUC: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})
æœ€ä½³é–¾å€¼: {best_threshold:.4f}

æ··æ·†çŸ©é™£:
{cm}

ğŸ“Š å¡æ–¹æª¢å®šçµæœ
-------------------
å›è³¼ç‹€æ…‹ Ã— ä»˜æ¬¾æ–¹å¼:
Ï‡Â² = {chi2_pay:.2f}, df = {dof_pay}, p = {p_pay:.4e}
CramÃ©r's V = {cramers_v:.4f}
çµè«–: {'é¡¯è‘—é—œè¯' if p_pay < 0.05 else 'ç„¡é¡¯è‘—é—œè¯'}

ğŸ” ä¸»è¦ç™¼ç¾
-------------------
"""

# åŠ å…¥é¡¯è‘—è®Šé …çš„è§£è®€
for _, row in sig_vars.iterrows():
    or_val = row['å‹ç®—æ¯” (OR)']
    if or_val > 1:
        summary += f"- {row['è®Šé …']}: OR={or_val:.3f}ï¼Œæ¯å¢åŠ 1å–®ä½ï¼Œå›è³¼å‹ç®—å¢åŠ  {(or_val-1)*100:.1f}%\n"
    else:
        summary += f"- {row['è®Šé …']}: OR={or_val:.3f}ï¼Œæ¯å¢åŠ 1å–®ä½ï¼Œå›è³¼å‹ç®—æ¸›å°‘ {(1-or_val)*100:.1f}%\n"

summary += f"""
ğŸ“ è¼¸å‡ºæª”æ¡ˆ
-------------------
âœ… {output_or}
âœ… {output_roc}
âœ… {output_cm}
âœ… {output_pr}

{'='*70}
"""

print(summary)

# å„²å­˜å ±å‘Š
report_path = os.path.join(output_folder, 'logistic_regression_report.txt')
with open(report_path, 'w', encoding='utf-8') as f:
    f.write(summary)

print(f"ğŸ“„ å ±å‘Šå·²å„²å­˜: {report_path}")
print("\nğŸ‰ é‚è¼¯æ–¯å›æ­¸åˆ†æå®Œæˆï¼")
